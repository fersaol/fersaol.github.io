<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title></title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
		<section id="header">
			<header>
				<span class="image avatar"> <img src="images\face.jpg" alt="mi foto"/></span>
				<h1 id="logo"><a href="index.html#one">Fernando Sánchez Olmo</a></h1>
				<p>Cloud Support Engineer and Data Analyst<br />
				Redescovering the World Everyday</p>
			</header>
			<nav id="nav">
				<ul>
					<li><a href="index.html#one" class="active">About Me</a></li>
					<li><a href="index.html#two">Things I Can Do</a></li>
					<li><a href="index.html#three">Portfolio</a></li>
					<li><a href="index.html#four">Contact</a></li>
				</ul>
			</nav>
			<footer>
				<ul class="icons">
					<li><a href="https://www.linkedin.com/in/fernandosanchezolmo/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
					<li><a href="https://github.com/fersaol" class="icon brands fa-github"><span class="label">Github</span></a></li>
					<li><a href="mailto:fersaolade@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
				</ul>
			</footer>
		</section>


<!-- Wrapper -->
    <div id="wrapper">

	<!-- Main -->
		<div id="main">
			<!-- One -->
				<section id="one">

					<div class="image main" data-position="center">
						<img src="images/co2mlpage_header.jpg"/>
					</div>

					<div class="container">
						<header class="major">
							<h2 align="center">CO2 Emissions Machine Learning Project</h2>
							<p>Applied Machine Learning for environmental policy driving<br />
							</p>
						</header>
						<p align="justify">This is the machine learning part that follows from CO2 EDA project. In this part we identify country
							clusters so we can then predict the country's efficiency while producing energy, based on the co2 emissions
							emitted by energy produced unit. Latly, we classify new data provided by this countries in order to make
							recommendations that allow them to drive its environmental policies. This project has been carried out by
							using non supervised machine learning and supervised machine learning.
							At the end, the three models are put together and devilered through webapps.
						<blockquote>For getting a full context of the project, please see the <a href="co2_eda.html"><b>CO2 EDA project</b></a>.</blockquote></p>
					</div>
				</section>
		</div>
	</div>


<!-- Wrapper2 -->
	<div id="wrapper">

		<!-- Main -->
			<div id="main">						
				<!-- two -->
					<section id="two">
						<div class="container" data-position="center">
							
							<section>
								<h3>CLUSTERING</h3>
								<p align="justify">First we take a look of the data, and we ca see that, not all, but 
									the great majority has a gamma distribution.</br> 
									<img src="images\cluster_images\raw_data_distribution.png" height="380" width="615"/></br>
									As the algorithms work better with Gaussian like 
									distributions, we scale them using Powertransformer, that makes a transformation 
									of the data first and then scales it with 0 mean and unit variance.</br>
									<img src="images\cluster_images\power_data_distribution.png" height="380" width="615"/></br>
									We also have a categorical feature that we would like to use into our models, so we preprocess
									this variable called Energy_type using the map fuction, as it allows us to decide the numbers 
									assigned to each label. This numbers will represent the co2 emission, so the greater the numbers
									the greater the co2 emitted by the energy source. We have gotten this assignation rule by obtaining
									the co2 emission mean of every energy source as we can see below:</br>
									<pre><code>df.groupby("Energy_type")["CO2_emission"].mean()</br></br>Energy_type</br>coal                        264.023</br>natural_gas                 111.226</br>nuclear                       0.000 </br>petroleum_and_other_liquids 208.812</br>renewables_and_others         0.000</br>Name: CO2_emission, dtype: float64</code></pre>
									<p align="justify">Once we have the data ready, we have made several trials with different features selections, hyperparameters 
									and two non supervised algorithms, KMeans and DBSCAN. The option that worked the best was a KMeans
									using the co2_emission and energy_production features as it had the best trade-off between simplicity and metrics
									performance, we can see the best result in the next graph(the dot size represents the gdp):</p>
									<img src="images\cluster_images\clusters_graph.png" height="300" width="615"/></br>
									This clusterization gives us the following country's categories:
									<img src="images\cluster_images\country_categories.png" height="300" width="615"/></br>
									where we can appreciate the following cluster's characteristics:
									<ul align="justify">
										<li><b>Cluster 0:</b>They are countries with a small energy production, but they does not have the smallest, mainly
										based on petroleum, natural gas and coal. Although they are the third world producers, their contamination is the second
										in the cluster's ranking so we could define them as not efficient countries with small productions and high co2 emissions
										for the production they have.</li>
										<li><b>Cluster 1:</b>This cluster has countries with low productions and low co2 emissions. They are not relevant countries 
										for the world energy production and contamination and are based on nuclear, renewables and natural gas mainly.</li>
										<li><b>Cluster 2:</b>This cluster has the world's biggest energy production countries and contaminating ones, in the graph above
										we can see that exists a huge different between the rest of the clusters. Its production is based on coal, natural gas and petroleum
										mainly,being coal the principal source, followed by petroleum and then natural gas. This three energy sources are the most contaminating
										of all the techologies taken into account, but the coal is above the rest with big difference.</li>
										<li><b>Cluster 3:</b>This cluster has countries with good energy productions, being the world's second producers, and low co2 emissions, 
										at the cluster 1 level for a greater production. We can also see that they use all the energy sources in a more or less proportional amount, 
										so their energy mix is very diversified and we could set this cluster out as the aim because of its very good eficiency.</li>
									</ul>
									<h4>¿Which metrics have we used to make our decisions?</h4>
									<h5>Elbow Method</h5>
									<p align="justify">This metric shows the sum of the cuadratic distances between each data point and its centroid, what is called inertia. Then, if we measure
										the inertia for different number of clusters we can visually determine what is the optimum by choosing the number where the decrease of the inertia begins
										to slow. For better understanding, we can determine a context where 1 cluster represents all data points and its inertia is maximum and the opposite where
										each data point is a cluster, so the inertia is 0. We will choose the better trade-off between inertia and number of clusters as we see in the graph below:</br>
										<img src="images\cluster_images\elbow.png" height="300" width="515"/></br>
										As we can see, the optimum is 4 clusters and also has the lower computational cost, what reinforces the choose.
									</p>
									<h5>Silhouette Coefficient</h5>
									The silhouette coefficient tells us if the algorithm is able to distinguish properly between the clusters. A good score of silhouette is above 0.5, and in our 
									dataset we can see the graphs below:
									<img src="images\cluster_images\silueta.png" height="300" width="615"/>
									<img src="images\cluster_images\silueta2.png" height="300" width="615"/></br>
									We can appreciate that 3 clusters is a good segmentation but 4 is better as its silhouette score is greater, meaning that 4 clusters represents better our data than 3.
									It is also possible to see that in both clusterizations all the clusters are above the mean score, so they are well shaped and they are clear.
								</p>
								<hr/>
								<h3>REGRESSION</h3>
								<P>In this section we have used the clusters to obtain 4 models which allows us to predict the country's eficiency. This eficiency is measured by the division of the co2 emission
									by the energy production, what give us the amount of co2 emitted by unit of energy production.</hr>
									The process has been divided into the following steps for each cluster:
									<ul>
									<li><b>Feature Selection by cluster:</b> In this part we have selected through different methods the features that better defines each cluster.
										The methods used have been the following:
									</li>
										<dt><i>- Correlation</i></dt>
										<dt><i>- RFECV (Recursive Feature Elimination Cross Validation)</i></dt>
										<dt><i>- VIF (Variance Inflator Factor)</i></dt>
										<dt><i>- Statsmodels</i></dt>
									<li><b>Cluster Base line</b></li>
									<li><b>Model Selection:</b> in this part we have used the following metrics:</li>
										<dt><i>- R2</i></dt>
										<dt><i>- MAE (Mean Absolute Error)</i></dt>
										<dt><i>- Median Absolute Error</i></dt>
										<dt><i>- Complexity</i></dt>
										<dd>In this part we have used the variables selected by each of the methods seen before in all the algorithms,
											so we could determine which combination of algorithm-variables works the best for each cluster.
										</dd>
									</ul>
							
								<h4>Cluster 0</h4>
								First of all we have created a base line with all the algorithms to have a reference point of comparison:</br>
								<img src="images\cluster_images\cluster0_bl.png" height="300" width="615"/></br>
								In this first glance we can see that the algorithms that better suites the grown truth are Ada Boost Regressor, 
								Random Forest Regressor, Gradient Boosting Regressor and Decision Tree Regressor. Now, we are going to take a look
								at the metrics of this base line, using all the variables:</br>
								<img src="images\cluster_images\cluster0_r2_bl.png" height="300" width="615"/></br>
								We can see that AdaBoost Regressor along with RandomForest Regressor are the ones that best represent the variability
								of the dependent variable, eficiency, as they have the highest scores. Also, we can see that its robustness or reliability
								are the greater, as they have the smallest standard deviation between scores gotten in the cross validation. Now we take a look
								at the Mean Square Error:
								<img src="images\cluster_images\cluster0_mse_bl.png" height="300" width="615"/></br>
								Here we can see that the best ones are the same, but although AdaBoost Regressor and Random Forest Regressor have the better metrics,
								they are not very reliable as they have a huge standard deviation, so it is possible that would be better choose GradientBossting Regressor
								if we had to choose at this time. Let's take a look at the mean absolute error:
								<img src="images\cluster_images\cluster0_mae_bl.png" height="300" width="615"/></br>
								In this graph we can appreciate that, as in the others, the best ones are the same algorithms and almost all of them have the same standar deviation
								between folds, so at these point we can intuit which one is going to be the selected.</br>
								In the last part, as we setted up before, we compare the metrics of all the algorithms between them and between feature selection method and finally we
								make the decision of choosing the AdaBoost Regressor based on Decision Tree Regressors because it has the best
								metric-complexity trade-off. The variables chosen were those selected by the RFCEV and Correlation methods, because both selected the same ones, being these:
								<ul>
									<li>Energy_production</li>
									<li>Energy_consumption</li>
									<li>Co2_emission</li>
									<li>balance</li>
								</ul>
								<h4>CLUSTER 0 MODEL SELECTION USING VARIABLES CHOSEN BY CORRELATION METHOD</h4>
								We can compare the reality versus prediction in the picture below so we can make a choice. The plot and the dataframe are part of a single function
								that sorts the models by the r2 metric and represents the first one(the data shown in the graph and the table are not exactly the same because when
								plotting the graph the model is corss validated ones again).
								<img src="images\cluster_images\cluster0_ada.png" height="300" width="615"/></br></br>
								<img src="images\cluster_images\cl0_panel_metricas.png" height="300" width="615"/></br></br>
								We see AdaBoostRegressor is the best, gathering the best r2 and median mae metric. We can also see that 
								the median is much lower than the mean absolute error what is telling us the extreme values are pulling 
								right the mean absolute error, so they are influencing the mean metric. In this sense, we are giving 
								more importance to the r2 and median metrics than the mean metric as decisory ones because we decided 
								we should keep the extreme values in the dataset because of the kind of data we are dealing off. 
								In consecuence, we want to minimize the effect of these data on the metrics.
								Let's now take a look at the validation curve for the model chosen so we can see if there is room for 
								improvement and other considerations:
								<img src="images\cluster_images\cl0_correlation_valcurve.png" height="300" width="615"/></br>

								We observe that the model learns well with training but does not do so well when generalizing in the 
								validation curve, reaching a maximum r2 value of approximately 0.68. This generates a fairly large 
								generalization error and indicates that the model suffers from variance or overfitting. Given this 
								diagnosis, we can see that with the increase in complexity it is not possible to improve the model 
								and therefore we have considered the following options:
								<ol>
									<li>The validation data is not sufficiently representative of the training dataset. This may be influenced by the extreme values we have maintained.</li>
									<li>We are trying to predict a simple matter with a too complex a model.</li>
								</ol>
								If we contrast this model, with a much simpler one such as Linear Regression we see that the metrics 
								it achieves are very poor, so it is likely to be more a problem of representativeness of the data 
								chosen for validation. In this sense, we increase the percentage of data for validation from 0.2 to 0.3 
								and maintain the method of cross validation using the RepetedKfold. This has improved the numbers 
								somewhat but the graph we obtained is maintained.
								Given these conclusions we consider that with the assumptions taken it is not possible to improve the 
								model much more at this point, so we are going to stay with it and try to improve it by adjusting the 
								hyperparameters of it. This way we are able to improve the model to 0.9322 r2 using a  hyperparameter
								n_estimators of 32 and the exponential value for loss. Then, we retrain the model and crossvalidate it
								as follows:
								<img src="images\cluster_images\cl0_hypermodel.png" height="300" width="615"/></br>
								We found that we have improved the generalization of the model at the expense of obtaining higher errors 
								as we can see in the metrics. That is, now our model has the ability to explain in a greater proportion 
								the variability of the target at the cost of incurring greater errors throughout its predictions. 
								This seems to us a good compromise given that in this situation our model is more reliable when predicting 
								with data that it has not seen because it indicates that it has better captured the general trend.
								Finally we represent the learning curve of this model:
								<img src="images\cluster_images\cl0_learningcurve.png" height="300" width="615"/></br>
								Observing the learning curve made with the Median Absolute Error metric, we observe that at the beginning 
								the model has a large BIAS but as the number of data increases, the training and validation curves are 
								approaching, converging towards a lower absolute median error for training and greater for validation in 
								order to reach an optimal point. However, once the maximum number of data contained in the dataset has 
								been reached, there is still a high generalization gap (distance between the validation curve and the 
								training curve) so they never converge, indicating that with a greater number of data it would still be 
								possible to improve this model.<br>
								<h4>CLUSTER 0 MODEL SELECTION USING VARIABLES CHOSEN BY VIF METHOD</h4>
								<img src="images\cluster_images\cl0_vif.png" height="300" width="615"/></br>
								In this model by selecting variables using the vif we can see that the estimator that best explains the 
								variability of the target is Random Forest Regressor with an approximately 0.74-0.76 r2 and a median error of 12.50 , 
								which difference from the average error is much lower. This indicates that, as in the previous case, there 
								are few errors in the data with extreme values but they affect the average enough due to its magnitude.
								<img src="images\cluster_images\cl0_vif_valcurve.png" height="300" width="615"/></br>
								We can see that there is some overfitting in the model, since although we see that the model learns well 
								in the training curve, the level of r2 that reaches the validation curve is a little far from that of 
								training, being between 0.80 and 0.85. Even so, we consider that it is an acceptable generalization gap 
								for the distribution of data that we have and as we have seen in the rest of the combinations.
								<img src="images\cluster_images\cl0_vif_hypermodel.png" height="300" width="615"/></br>
								We see that we have been able to improve the ability to explain the variability of the target by reducing 
								the average absolute error and increasing the median absolute error somewhat. It is also a model that has 
								a much lower rmse so the large errors of extreme values are affecting it less and it is a model that in 
								general has a better ability to generalize than that of its base line.
								<img src="images\cluster_images\cl0_vif_learningcurve.png" height="300" width="615"/></br>
								<img src="images\cluster_images\cl0_vif_learningcurve2.png" height="300" width="615"/></br>
								Looking at the learning curve in which we use as a metric the square root of the mean square error we can 
								see that as the number of data increases, the curves are approaching, but they never converge so we could 
								establish that with a greater number of data it would be possible to improve the model. However, given the 
								characteristics of our data, we consider that it is more reliable to use the median of absolute error and 
								therefore we perform its learning curve in order to confirm the previous conclusions. In this sense, we 
								observe that the lines converge in a more pronounced but not sufficient way and therefore our previous 
								assumptions are confirmed.
								<h4>CLUSTER 0 MODEL SELECTION USING VARIABLES CHOSEN BY RFECV METHOD</h4>
								<img src="images\cluster_images\cl0_rfecv_model.png" height="300" width="615"/></br>
								We see that in this case the best estimator is GradientBoosting Regressor with an r2 of 0.8014, 
								a mae of 544.16 and a median absolute error of 48.65.
								<img src="images\cluster_images\cl0_rfecv_valcurve.png" height="300" width="615"/></br>
								In the validation curve we see how there is a discrepancy between the value of the training curve 
								and the validation curve, having a large generalization gap between them, and therefore that, the 
								model is able to predict, but not very well with new data. 
								Given the previous study, which obtained much better results, we deduced that the configuration of 
								the model with the variables chosen by this method makes it not the most appropriate, since it does 
								not correctly capture the general trend of the data set. Still, let's try to improve it somewhat with 
								the adjustment of its hyperparameters where we have been able to achieve a 0.8414 r2 and plot it:
								<img src="images\cluster_images\cl0_rfecv_hypermod.png" height="300" width="615"/></br>
								Given the validation curve and the conclusions, we thought that the improvement to be achieved, would 
								not be very high as it has been, however we have improved more than we expected.
								We can see that with our model adjusted by hyperparameters we obtain a higher value of r2, with which 
								we better explain the variability of our target through its independent variables and the median error 
								has been reduced by more than half, so it is a better model than the initial one. Let's look at the 
								lerning curve:
								<img src="images\cluster_images\cl0_rfecv_Learningcurve.png" height="300" width="615"/></br>
								In the learning curve we see how as we add new data the model is doing better and better when it comes 
								to generalizing, reducing the bias and increasing the variance. However, with the data that we have, we 
								do not get a model that converges completely, and therefore, adding new data would improve it. Still, we 
								consider the median error not to be extremely high, although we've found other models that do better.
								<h4>CLUSTER 0 MODEL SELECTION USING VARIABLES CHOSEN BY OLS-STATSMODELS METHOD</h4>
								<img src="images\cluster_images\cl0_ols_model.png" height="300" width="615"/></br>
								In this model configuration we can observe that AdaBoost Regressor has achieve the best R2 and Median 
								absolute error value. As we can see in the graph the extreme values influences the mean absolute error. 
								Anyways, we choose the best model and take a look at the validation curve so we can explore the 
								improvement options using it:
								<img src="images\cluster_images\cl0_ols_valcurve.png" height="300" width="615"/></br>
								As in the other configurations where the selected model has been AdaBoost Regressor we see that the 
								validation curve is irregular along its pace as the complexity increases and that the validation curve 
								is not able to reach an r2 value close to the training one. This indicates that the model learns well 
								but it struggles at predicting new data not seen before so the model suffers from overfitting. As this 
								method is the last and seen the before conclusions we think this results are due to the extreme values 
								kept. 
								<img src="images\cluster_images\cl0_ols_hypermod.png" height="300" width="615"/></br>
								After several attempts for hyperparameter tuning, we managed to improve the model somewhat obtaining a 
								0.852 r2. All of them have been around an r2 of 0.80 so, given the validation curve we obtained, we consider 
								the improvement good and that this model does not have much more room for improvement.
								<img src="images\cluster_images\cl0_ols_learningcurve.png" height="300" width="615"/></br>
								If we see the learning curve made using r2 score we see the model converges but at a score very low. 
								Seen this, we plot the same curve but using the median absolute error:
								<img src="images\cluster_images\cl0_ols_learningCurve2.png" height="300" width="615"/></br>
								<h3>CLUSTER 0 CONCLUSION</h3>
								We can see that the model made with the variables obtained by the correlation method is better in the 
								r2 metrics and in complexity, but obtains a very high median error with respect to the second best in 
								r2 and the first in median error, which is the model made with the variables selected by vif.In this 
								sense, we consider that although the complexity of the model is increased by twice choosing the vif, 
								with it we are explaining practically the same percentage of the variability of the dependent variable, 
								reducing the absolute median error by a much higher percentage, and therefore turns out to be the 
								chosen model. In this way, we retrain the model using all the data and save it for the production phase.<hr>

								<blockquote><b>We follow the same reasoning for the rest of the clusters so you can check this link out
												,<a href="https://github.com/fersaol/The_Bridge_CO2_ML_Project/blob/main/src/notebooks/Regression_byClusters.ipynb">regression by cluster</a>, for further information about the rest of clusters</b></blockquote>
								</p>
								<hr/>
								<h3>CLASSIFICATION</h3>
								<p align="justify">In this part we perform the classification of new data given into one of the preceding clusters obtained.
									For that, we have removed the variables that we used in the clusterization part in order to not allow
									the algorithms use them, as if we do, they will use them to obtein almost perfect results due to the clusters
									were made by those variables, and the algorithms choose only those two variables.
									First of all we check the class balance in our dataset as we can see in the graph below:</br>
									<img src="images\cluster_images\class_balance.png" height="300" width="660"/></br>
									Although the classes are imbalanced we do not consider is in a scale enough to justify a resampling, so we have kept the 
									data structure.</br>
									<h4>Base Line</h4>
									We create our metric's base line as follow:</br>
									<img src="images\cluster_images\accuracy.png" height="300" width="660"/></br>
									<img src="images\cluster_images\precision.png" height="300" width="660"/></br>
									<img src="images\cluster_images\recall.png" height="300" width="660"/></br>
									<img src="images\cluster_images\f1.png" height="300" width="660"/></br>
									In general we can see that they perform in the same order in all the metrics with little variations in values and standard deviation,
									being the three best Random Forest Classifier, Gradient Boosting Classifier and Decision Tree Clasifier, what makes us think the choose 
									is going to be between these three.
									<h4>Features Selection</h4>
									As in the previous parts we have gotten very good results using RFECV, this is the only method we are using for selecting variables in this
									part. The results for the best three estimators in the base line has been:
									<pre><code>vars_RandomForest= ['GDP', 'Population', 'Energy_consumption', 'per_capita_production','Energy_intensity_by_GDP', 'balance', 'energy_dependecy', 'co2_pc','energy_type']
vars_xgb = ['Population', 'Energy_consumption', 'per_capita_production','balance', 'co2_pc', 'energy_type']
vars_DecisionTree = ['GDP', 'Population', 'Energy_consumption', 'per_capita_production','balance', 'co2_pc', 'energy_type']</code></pre>
									
									<h4>Metrics by Estimator</h4>
									Now we look at the general metrics for each estimator and then by estimator and cluster to decide which is best:
									<h5>Metrics by estimator cross validated</h5>
									Here we can see the cross validated metrics for each estimator selected:
									<h6>Random Forest Metrics</h6>
									<img src="images\cluster_images\rf_metrics.png" height="200" width="415"/></br>
									<h6>Gradient Boosting Metrics</h6>
									<img src="images\cluster_images\gb_metrics.png" height="200" width="415"/></br>
									<h6>Decision Tree Metrics</h6>
									<img src="images\cluster_images\dt_metrics.png" height="200" width="415"/></br>
									In general all of them are good classifying, but Random Forest is the best in all of the metrics, so in general this is the best estimator. Let's see the metrics for each estimator
									and cluster, so we can know where the classifiers do its job better and worse:
									<h5>Metrics by Cluster</h5>
									<img src="images\cluster_images\precisionbycluster.png" height="300" width="660"/></br>
									<img src="images\cluster_images\recallbycluster.png" height="300" width="660"/></br>
									<img src="images\cluster_images\f1bycluster.png" height="300" width="660"/></br>
									If we look at the precision metric we can see that for cluster 0 and cluster 2 the best estimator is Random Forest, for cluster 1 is
									Decision Tree and for cluster 3 Gradient Boosting. This means that each one of the estimators chosen are the best in detecting the right
									cluster among the positive predictions.</br>
									In the other hand, looking at the recall metric, that shows the quantity of right prediction among the real number of positive clusters, we can see that
									Random Forest is the best in almost every cluster apart from cluster 3 where all of them do it the same.</br>
									As Random Forest has done it better than the others in most of the clusters, we expect that it does well in f1 micro metric for all
									the clusters, and so it does. This is because f1 is a composite metric calculated by the harmonic mean of precision and recall.
									<h4>Conclusion</h4>
									If we compare the mean metrics from the cross validation we can appreciate that:
										<ol>
											<li>Random Forest has the best accuracy, 0.978 and deviation +/- 0.003</li>
											<li>Random Forest has the best precision, 0.977 and deviation +/- 0.003</li>
											<li>Random Forest has the best recall, 0.978 and deviation +/- 0.003</li>
											<li>Random Forest has the best f1 score, 0.977 and deviation +/- 0.003</li>
										</ol>
									Because of these, the chosen estimator has been Random Forest Classifier as it has the best metrics with the least standard deviation between predictions. 
							
								</p>
								<hr/>
								<h3>PCA (Principal Components Analysis)</h3>
								<p></p>
								<hr/>
								<h3>PRODUCTION-WEB APP</h3>
								<p>I you click on the buttons below you could check the webapps out and make your own simulations:</p>
								
								<ul class="actions">
									<li><a href="https://fersaol-co2-streamlit-webapp-app-uh1z3w.streamlitapp.com/" class="button primary">Streamlit WebApp</a></li>
									<li><a href="https://co2-ml-app.herokuapp.com/" class="button primary">Flask WebApp</a></li>
								</ul>

								<ul class="icons">
									<h5>See the code on GitHub</h5>
									<li><a href="https://github.com/fersaol/The_Bridge_CO2_ML_Project" class="icon brands fa-github"><span class="label">Github</span></a></li>
								</ul>
							</section>	
						</div>	
			<!-- Footer -->
			<section id="footer">
				<div class="container">
					<ul class="copyright">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</section>
		</div>
	</div>

	<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrollex.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

</body>
    </html>